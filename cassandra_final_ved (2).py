# -*- coding: utf-8 -*-
"""Cassandra_Final_ved.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rAuYJInNe3pSMAyBm5u4LYDilE04B8Ia
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objs as go
import plotly.offline as pyoff

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/cassandra/trainData.csv')

df.head()

df.dtypes

len(df)

df.isnull().sum()

df.drop('Description', axis = 1, inplace = True)

indexNames = df[(df['Number_of_Days_until_Payment'] < 0)].index
df.drop(indexNames , inplace=True)

from datetime import time, date, datetime

def convertInt(df):
  df['Amount'] = df['Amount'].astype('int')
  df['Settled'] = df['Settled'].astype('int')
  return df


def convertDate(df):
  # df['Hours'] = pd.to_datetime(df['Created']).dt.time
  # df['Hours'] = df.Hours.apply(lambda x: x.hour)
  # df['Hours']=df['Hours'].astype(int)
  # df['Created'] = pd.to_datetime(df['Created']).dt.date
  df['Invoice_Date'] = pd.to_datetime(df['Invoice_Date']).dt.date
  df['Due_Date'] = pd.to_datetime(df['Due_Date']).dt.date
  df['Deadline'] = (df['Due_Date'] - df['Invoice_Date']).dt.days
  df['Created'] = pd.to_datetime(df['Created']).dt.date
  df.drop(['Created'], axis=1, inplace=True)
  return df


def processDate(df):
    
    df['Invoice_Date'] = pd.to_datetime(df['Invoice_Date'])
    # df['year_i'] = pd.to_datetime(df['Invoice_Date']).dt.year
    # df['month_i'] = pd.to_datetime(df['Invoice_Date']).dt.month
    df['Latest_Payment_Date'] = df['Due_Date']
    # df['Latest_Payment_Date'] = df['Invoice_Date'].apply(lambda x : x.dayofweek)
    df.drop(['Invoice_Date'], axis=1, inplace=True)

    # df['Due_Date'] = pd.to_datetime(df['Due_Date'])
    # df['year_d'] = pd.to_datetime(df['Due_Date']).dt.year
    # df['month_d'] = pd.to_datetime(df['Due_Date']).dt.month
    # df['day_d'] = df['Due_Date'].apply(lambda x : x.dayofweek)
    df.drop(['Due_Date'], axis=1, inplace=True)
    return df


def getDataframe(Vendor_Name, pred):
    sub=pd.DataFrame([Vendor_Name, pred])
    sub=sub.transpose()
    sub.columns=['Vendor_Name', 'Number_of_Days_until_Payment']
    sub['Number_of_Days_until_Payment'] = sub['Number_of_Days_until_Payment'].apply(np.floor)
    sub['Vendor_Name']=sub['Vendor_Name'].astype(object)
    sub.to_csv('Submission9.csv', index=False)
    
    return sub

convertDate(df)

convertInt(df)

processDate(df)

df.dtypes

#create a generic user dataframe to keep Vendor_Name and new segmentation scores
dfr = pd.DataFrame(df['Vendor_Name'].unique()) #dfr = df recency
dfr.columns = ['Vendor_Name']

#get the max purchase date for each customer and create a dataframe with it
dfr2 = df.groupby('Vendor_Name').Latest_Payment_Date.max().reset_index()
dfr2.columns = ['Vendor_Name','Recent_Payment_Date']
#we take our observation point as the max recent purchase date in our dataset
dfr2['Recency'] = (dfr2['Recent_Payment_Date'].max() - dfr2['Recent_Payment_Date']).dt.days

# #merge this dataframe to our new user dataframe
dfr = pd.merge(dfr, dfr2[['Vendor_Name','Recency']], on='Vendor_Name')
dfr

plot_data = [
    go.Histogram(
        x=dfr['Recency']
    )
]

plot_layout = go.Layout(
        title='Recency'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

from sklearn.cluster import KMeans

sse={}
tx_recency = dfr[['Recency']]
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)
    tx_recency["clusters"] = kmeans.labels_
    sse[k] = kmeans.inertia_ 
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.show()

#build 4 clusters for recency and add it to dataframe
kmeans = KMeans(n_clusters=4)
kmeans.fit(dfr[['Recency']])
dfr['RecencyCluster'] = kmeans.predict(dfr[['Recency']])

#function for ordering cluster numbers...0 means worst(most old customer) and 3 means best(most recent)
def order_cluster(cluster_field_name, target_field_name,df,ascending):
    new_cluster_field_name = 'new_' + cluster_field_name
    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()
    df_new = df_new.sort_values(by = target_field_name,ascending = ascending).reset_index(drop=True)
    df_new['index'] = df_new.index
    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)
    df_final = df_final.drop([cluster_field_name],axis=1)
    df_final = df_final.rename(columns={"index":cluster_field_name})
    return df_final

dfr = order_cluster('RecencyCluster', 'Recency',dfr,False)
dfr

dfr.groupby('RecencyCluster').sum()

#Latest_Payment_Date means when he made the most recent payment
#higher frequency means pays regularly
df_freq = df.groupby('Vendor_Name').Latest_Payment_Date.count().reset_index()
df_freq.columns = ['Vendor_Name','Frequency']

#add this data to our main dataframe
dfr = pd.merge(dfr, df_freq, on='Vendor_Name')

dfr

#plot the histogram
plot_data = [
    go.Histogram(
        x = dfr['Frequency']
    )
]

plot_layout = go.Layout(
        title='Frequency'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

from sklearn.cluster import KMeans

ssee={}
tx_Frequency = dfr[['Frequency']]
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_Frequency)
    tx_Frequency["clusters"] = kmeans.labels_
    ssee[k] = kmeans.inertia_ 
plt.figure()
plt.plot(list(ssee.keys()), list(ssee.values()))
plt.xlabel("Number of cluster")
plt.show()

#k-means
kmeans = KMeans(n_clusters=4)
kmeans.fit(dfr[['Frequency']])
dfr['FrequencyCluster'] = kmeans.predict(dfr[['Frequency']])

#order the frequency cluster
dfr = order_cluster('FrequencyCluster', 'Frequency',dfr,True) 

#see details of each cluster....0 means least frequent in paying the bills, 3 means most frequent in paying the bill
dfr

dfr.groupby('FrequencyCluster')['Frequency'].describe()

#thought of outstanding

df['Binary_Outstanding'] = np.where(((df.Outstanding)>0), 1, 0)  #if outstanding, then do +1 for that user...later subtract from overall score
tx_Outstanding = df.groupby('Vendor_Name').Binary_Outstanding.sum().reset_index()

#merge it with our main dataframe
dfr = pd.merge(dfr, tx_Outstanding, on='Vendor_Name')
dfr

#plot the histogram
plot_data = [
    go.Histogram(
        x = dfr['Binary_Outstanding']
    )
]

plot_layout = go.Layout(
        title='Outstanding count'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

#apply clustering....higher OutstandingCluster means most no. of outstanding
kmeans = KMeans(n_clusters=4)
kmeans.fit(dfr[['Binary_Outstanding']])
dfr['OutstandingCluster'] = kmeans.predict(dfr[['Binary_Outstanding']])


#order the cluster numbers
dfr = order_cluster('OutstandingCluster', 'Binary_Outstanding',dfr,True)

#show details of the dataframe
dfr

dfr.groupby('OutstandingCluster')['Binary_Outstanding'].describe()

dfr['OverallScore'] = dfr['RecencyCluster'] + dfr['FrequencyCluster'] - dfr['OutstandingCluster']  #subtract outstanding cluster
dfr

dfr.groupby('OverallScore')['Recency','Frequency','Binary_Outstanding'].mean()

#RFM DONE!

df_no_of_days_for_payment = df[['Vendor_Name','Number_of_Days_until_Payment','Latest_Payment_Date']]
df_no_of_days_for_payment = df_no_of_days_for_payment.sort_values(['Vendor_Name','Latest_Payment_Date'])
df_no_of_days_for_payment

df_no_of_days_for_payment.drop('Latest_Payment_Date', axis=1, inplace=True)

#shifting last 3 purchase dates
df_no_of_days_for_payment['P1_Days_until_Payment'] = df_no_of_days_for_payment.groupby('Vendor_Name')['Number_of_Days_until_Payment'].shift(1)
df_no_of_days_for_payment['P2_Days_until_Payment'] = df_no_of_days_for_payment.groupby('Vendor_Name')['Number_of_Days_until_Payment'].shift(2)
df_no_of_days_for_payment

# df_no_of_days_for_payment['mean'] = df_no_of_days_for_payment['Vendor_Name'].mean(axis=1,skipna=True)
df_no_of_days_for_payment['mean'] = df_no_of_days_for_payment[['Number_of_Days_until_Payment', 'P1_Days_until_Payment','P2_Days_until_Payment']].mean(axis=1,skipna=True)
df_no_of_days_for_payment

# df_no_of_days_for_payment['st_deviation'] = df_no_of_days_for_payment[['Number_of_Days_until_Payment', 'P1_Days_until_Payment','P2_Days_until_Payment']].std(axis=1,skipna=True)
# df_no_of_days_for_payment

df_no_of_days_for_payment_LAST = df_no_of_days_for_payment.drop_duplicates(subset=['Vendor_Name'],keep='last')
df_no_of_days_for_payment_LAST

dfr = pd.merge(dfr, df_no_of_days_for_payment_LAST[['Vendor_Name','Number_of_Days_until_Payment','P1_Days_until_Payment','P2_Days_until_Payment','mean']], on='Vendor_Name')
dfr

# dfr.Number_of_Days_until_Payment.describe()
dfr.drop(dfr.columns[[8, 9, 10]], axis = 1, inplace = True)
dfr.rename(columns = {'mean':'Number_of_Days_until_Payment'}, inplace = True)  #############remember##################
dfr

label = dfr['Number_of_Days_until_Payment']
label

# # delete all rows for which column 'Number_of_Days_until_Payment' has -ve value
# indexNames = dfr[(dfr['Number_of_Days_until_Payment'] < 0)].index
# dfr.drop(indexNames , inplace=True)

dfr_class = dfr.copy()

corr = dfr_class[dfr_class.columns].corr()
plt.figure(figsize = (30,20))
sns.heatmap(corr, annot = True, linewidths=0.2, fmt=".2f")

# dfr_class['Days_until_Payment_Range'] = 3
# dfr_class.loc[dfr_class.Number_of_Days_until_Payment>20,'Days_until_Payment_Range'] = 2
# dfr_class.loc[dfr_class.Number_of_Days_until_Payment>50,'Days_until_Payment_Range'] = 1
# dfr_class.loc[dfr_class.Number_of_Days_until_Payment>90,'Days_until_Payment_Range'] = 0
dfr_class

# dfr_class.drop(dfr_class.columns[[8, 9, 10]], axis = 1, inplace = True)
# dfr_class
# label = dfr_class['Number_of_Days_until_Payment']
# print(label)

# dfr_class = pd.get_dummies(dfr_class)
# dfr_class

dfr_class = dfr_class.drop('Number_of_Days_until_Payment',axis=1)

#model



# from sklearn.svm import SVC
# from sklearn.multioutput import MultiOutputClassifier
# from sklearn.ensemble import GradientBoostingClassifier
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.naive_bayes import GaussianNB
# from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
# from sklearn.linear_model import LogisticRegression
# import xgboost as xgb
# from sklearn.model_selection import StandardScaler, KFold, cross_val_score, train_test_split
# #train & test split
# # dfr_class = dfr_class.drop('Number_of_Days_until_Payment',axis=1)
# X, y = dfr_class, label
# x1, x2, y1, y2 = train_test_split(X, y, test_size=0.2, random_state=44)

# #create an array of models
# # models = []
# # models.append(("LR",LogisticRegression()))
# # models.append(("NB",GaussianNB()))
# # models.append(("RF",RandomForestClassifier()))
# # models.append(("SVC",SVC()))
# # models.append(("Dtree",DecisionTreeClassifier()))
# # models.append(("XGB",xgb.XGBClassifier()))
# # models.append(("KNN",KNeighborsClassifier()))

# # #measure the accuracy 
# # for name,model in models:
# #     kfold = KFold(n_splits=2)
# #     cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = "accuracy")
# #     print(name, cv_result)
# # model = xgb.XGBClassifier()
# # model.fit(X_train, y_train)


# # x1, x2, y1, y2 = train_test_split(x_train, label, test_size=0.1, random_state=42)

# sc = StandardScaler()
# X_train_std = sc.fit_transform(x1)#doubt#
# # X_test_std = sc.transform(x2)
# X_test_std = sc.transform(x2)



DF = pd.read_csv('/content/drive/MyDrive/cassandra/testData.csv')
X_test=DF
label2 = X_test['Vendor_Name'].astype(object)
# data=DF
# Vendor_Name = X_test['Vendor_Name'].astype(object)
X_test.drop(['Description'], axis=1, inplace=True)

X_test = convertDate(X_test)
X_test = convertInt(X_test)
X_test = processDate(X_test)

#create a generic user dataframe to keep Vendor_Name and new segmentation scores
X_test_r = pd.DataFrame(X_test['Vendor_Name'].unique()) #dfr = df recency
X_test_r.columns = ['Vendor_Name']

#get the max purchase date for each customer and create a dataframe with it
X_test_r_2 = X_test.groupby('Vendor_Name').Latest_Payment_Date.max().reset_index()
X_test_r_2.columns = ['Vendor_Name','Recent_Payment_Date']
#we take our observation point as the max recent purchase date in our dataset
X_test_r_2['Recency'] = (X_test_r_2['Recent_Payment_Date'].max() - X_test_r_2['Recent_Payment_Date']).dt.days

# #merge this dataframe to our new user dataframe
X_test_r = pd.merge(X_test_r, X_test_r_2[['Vendor_Name','Recency']], on='Vendor_Name')
X_test_r

#build 4 clusters for recency and add it to dataframe
kmeans = KMeans(n_clusters=4)
kmeans.fit(X_test_r[['Recency']])
X_test_r['RecencyCluster'] = kmeans.predict(X_test_r[['Recency']])

#function for ordering cluster numbers...0 means worst(most old customer) and 3 means best(most recent)
def order_cluster(cluster_field_name, target_field_name,df,ascending):
    new_cluster_field_name = 'new_' + cluster_field_name
    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()
    df_new = df_new.sort_values(by = target_field_name,ascending = ascending).reset_index(drop=True)
    df_new['index'] = df_new.index
    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)
    df_final = df_final.drop([cluster_field_name],axis=1)
    df_final = df_final.rename(columns={"index":cluster_field_name})
    return df_final

X_test_r = order_cluster('RecencyCluster', 'Recency',X_test_r,False)
X_test_r

#Latest_Payment_Date means when he made the most recent payment
#higher frequency means pays regularly
X_test_freq = X_test.groupby('Vendor_Name').Latest_Payment_Date.count().reset_index()
X_test_freq.columns = ['Vendor_Name','Frequency']

#add this data to our main dataframe
X_test_r = pd.merge(X_test_r, X_test_freq, on='Vendor_Name')

X_test_r

#k-means
kmeans = KMeans(n_clusters=4)
kmeans.fit(X_test_r[['Frequency']])
X_test_r['FrequencyCluster'] = kmeans.predict(X_test_r[['Frequency']])

#order the frequency cluster
X_test_r = order_cluster('FrequencyCluster', 'Frequency',X_test_r,True) 

#see details of each cluster....0 means least frequent in paying the bills, 3 means most frequent in paying the bill
X_test_r

#thought of outstanding

X_test['Binary_Outstanding'] = np.where(((X_test.Outstanding)>0), 1, 0)  #if outstanding, then do +1 for that user...later subtract from overall score
X_test_Outstanding = df.groupby('Vendor_Name').Binary_Outstanding.sum().reset_index()

#merge it with our main dataframe
X_test_r = pd.merge(X_test_r, X_test_Outstanding, on='Vendor_Name')
X_test_r

#apply clustering....higher OutstandingCluster means most no. of outstanding
kmeans = KMeans(n_clusters=4)
kmeans.fit(X_test_r[['Binary_Outstanding']])
X_test_r['OutstandingCluster'] = kmeans.predict(X_test_r[['Binary_Outstanding']])


#order the cluster numbers
X_test_r = order_cluster('OutstandingCluster', 'Binary_Outstanding',X_test_r,True)

#show details of the dataframe
X_test_r

X_test_r['OverallScore'] = X_test_r['RecencyCluster'] + X_test_r['FrequencyCluster'] - X_test_r['OutstandingCluster']  #subtract outstanding cluster
X_test_r

X_test_r_class = X_test_r.copy()

# X_test_r_class = pd.get_dummies(X_test_r_class)
X_test_r_class

# #train & test split
# # dfr_class = dfr_class.drop('Number_of_Days_until_Payment',axis=1)
# # X, y = X_test_r_class
# # X_tr, X_te, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)

# # #create an array of models
# # models = []
# # models.append(("LR",LogisticRegression()))
# # models.append(("NB",GaussianNB()))
# # models.append(("RF",RandomForestClassifier()))
# # models.append(("SVC",SVC()))
# # models.append(("Dtree",DecisionTreeClassifier()))
# # models.append(("XGB",xgb.XGBClassifier()))
# # models.append(("KNN",KNeighborsClassifier()))

# # #measure the accuracy 
# # for name,model in models:
# #     kfold = KFold(n_splits=2)
# #     cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = "accuracy")
# #     print(name, cv_result)

# y_pred = model.predict(X_test_r_class)
# # y_pred = model.predict(X_test)
# # predictions = [round(value) for value in y_pred]
# print(getDataframe(label2,y_pred))

X_test_r_class.drop('Vendor_Name', axis=1, inplace=True)

dfr_class.drop('Vendor_Name', axis=1, inplace=True)



from sklearn.svm import SVC
from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
#train & test split
# dfr_class = dfr_class.drop('Number_of_Days_until_Payment',axis=1)
X, y = dfr_class, label
x1, x2, y1, y2 = train_test_split(X, y, test_size=0.2, random_state=44)

#create an array of models
# models = []
# models.append(("LR",LogisticRegression()))
# models.append(("NB",GaussianNB()))
# models.append(("RF",RandomForestClassifier()))
# models.append(("SVC",SVC()))
# models.append(("Dtree",DecisionTreeClassifier()))
# models.append(("XGB",xgb.XGBClassifier()))
# models.append(("KNN",KNeighborsClassifier()))

# #measure the accuracy 
# for name,model in models:
#     kfold = KFold(n_splits=2)
#     cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = "accuracy")
#     print(name, cv_result)
# model = xgb.XGBClassifier()
# model.fit(X_train, y_train)


# x1, x2, y1, y2 = train_test_split(x_train, label, test_size=0.1, random_state=42)

sc = StandardScaler()
X_train_std = sc.fit_transform(x1)#doubt#
# X_test_std = sc.transform(x2)
X_test_std = sc.transform(x2)

gbr_params = {'n_estimators': 1000,
          'max_depth': 15,
          'min_samples_split': 5,
          'learning_rate': 0.07,
          'loss': 'squared_error'}

gbr = GradientBoostingRegressor(**gbr_params)
gbr.fit(X_train_std, y1)

print(gbr.score(X_test_std, y2))
X_std = sc.transform(X_test_r_class)
pred=gbr.predict(X_std)
# pred = pred.astype(int)
print(getDataframe(label2, pred))

# x_train=df
# # x_train.drop(['Number_of_Days_until_Payment'], axis=1, inplace=True)
# x_train.drop(['Vendor_Name', 'Number_of_Days_until_Payment' ], axis=1, inplace=True)
# x_train.head()

# DF=pd.read_csv('/content/drive/MyDrive/cassandra/testData.csv')
# X_test=DF
# data=DF
# Vendor_Name =X_test['Vendor_Name'].astype(object)
# X_test.drop(['Description'], axis=1, inplace=True)
# X_test.drop(['Vendor_Name'], axis=1, inplace=True)

# Vendor_Name

# X_test.head()

# X_test = convertDate(X_test)
# X_test=processDate(X_test)
# # X_test=processDate_C(X_test)
# X_test = convertInt(X_test)

# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.preprocessing import StandardScaler

# x1, x2, y1, y2 = train_test_split(x_train, label, test_size=0.1, random_state=42)

# sc = StandardScaler()
# X_train_std = sc.fit_transform(x1)#doubt#
# # X_test_std = sc.transform(x2)
# X_test_std = sc.transform(x2)

# from sklearn.model_selection import cross_val_score

# model = RandomForestRegressor()
# cross_val_score(model,x_train,label)

# gbr_params = {'n_estimators': 1000,
#           'max_depth': 15,
#           'min_samples_split': 5,
#           'learning_rate': 0.07,
#           'loss': 'squared_error'}

# gbr = GradientBoostingRegressor(**gbr_params)
# gbr.fit(X_train_std, y1)

# print(gbr.score(X_test_std, y2))
# X_std = sc.transform(X_test)
# pred = gbr.predict(X_std)
# # pred = pred.astype(int)
# print(getDataframe(Vendor_Name, pred))

# DF=pd.read_csv('/content/drive/MyDrive/cassandra/testData.csv')
# X_test=DF
# data=DF
# Vendor_Name =X_test['Vendor_Name'].astype(object)
# X_test.drop(['Description'], axis=1, inplace=True)
# X_test.drop(['Vendor_Name'], axis=1, inplace=True)